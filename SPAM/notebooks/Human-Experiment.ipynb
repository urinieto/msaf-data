{
 "metadata": {
  "name": "",
  "signature": "sha256:d64e03192c03c755edd2724cc25ccbae18df2ecfcf2a014ac4dd717c67b6610c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Add last boundary to human annotations\n",
      "from msaf import jams2\n",
      "import glob\n",
      "import json\n",
      "\n",
      "annotators = {}\n",
      "annotators[\"Colin\"] = {\n",
      "    \"name\"  : \"Colin\",\n",
      "    \"email\" : \"colin.z.hua@gmail.com\"\n",
      "}\n",
      "annotators[\"Eleni\"] = {\n",
      "    \"name\"  : \"Eleni\",\n",
      "    \"email\" : \"evm241@nyu.edu\"\n",
      "}\n",
      "annotators[\"Evan\"] = {\n",
      "    \"name\"  : \"Evan\",\n",
      "    \"email\" : \"esj254@nyu.edu\"\n",
      "}\n",
      "annotators[\"John\"] = {\n",
      "    \"name\"  : \"John\",\n",
      "    \"email\" : \"johnturner@me.com\"\n",
      "}\n",
      "annotators[\"Shuli\"] = {\n",
      "    \"name\"  : \"Shuli\",\n",
      "    \"email\" : \"luiseslt@gmail.com\"\n",
      "}\n",
      "\n",
      "def update_last_boundary(jam_file, annotator_name, time, context):\n",
      "    jam = jams2.load(jam_file)\n",
      "    for annotation in jam[\"sections\"]:\n",
      "        if annotation.annotation_metadata.annotator.name == annotator_name:\n",
      "            for i, data in enumerate(annotation.data):\n",
      "                if annotation.data[i+1].label.context == \"small_scale\":\n",
      "                    data.end.value = time\n",
      "                    break\n",
      "            break\n",
      "    json.dump(jam, open(jam_file, \"w\"), indent=2)\n",
      "    \n",
      "def add_last_boundary(jam_file, annotator_name, time, context):\n",
      "    jam = jams2.load(jam_file)\n",
      "    for annotation in jam[\"sections\"]:\n",
      "        if annotation.annotation_metadata.annotator.name == annotator_name:\n",
      "            for i, data in enumerate(annotation.data):\n",
      "                if annotation.data[i+1].label.context == \"small_scale\":\n",
      "                    segment = annotation.create_datapoint()\n",
      "                    segment.start.value = data.end.value\n",
      "                    segment.end.value = time\n",
      "                    segment.label.value = \"END\"\n",
      "                    segment.label.context = context\n",
      "                    break\n",
      "            break\n",
      "    json.dump(jam, open(jam_file, \"w\"), indent=2)\n",
      "\n",
      "jam_files = glob.glob(\"/Users/uri/datasets/SubSegments/annotations/*.jams\")\n",
      "context = \"large_scale\"\n",
      "for jam_file in jam_files:\n",
      "    jam = jams2.load(jam_file)\n",
      "    dur = jam.metadata.duration\n",
      "    for key in annotators.keys():\n",
      "        inters, labels = jams2.converters.load_jams_range(jam_file, \"sections\", \n",
      "                                                          annotator_name=annotators[key][\"name\"], \n",
      "                                                          context=context)\n",
      "        if dur - inters[-1,-1] < -.1:\n",
      "            print \"Warning: the last boundary by %s is placed after the track duration! (%s)\" % \\\n",
      "                (key, jam_file)\n",
      "            print dur, inters[-1, -1]\n",
      "            update_last_boundary(jam_file, key, dur, context)\n",
      "        \n",
      "        if np.abs(dur - inters[-1, -1]) > 0.5:\n",
      "            print \"Warning: the last boundary by %s is not placed at the end of the track! (%s)\" % \\\n",
      "                (key, jam_file) \n",
      "            print dur, inters[-1, -1]\n",
      "            add_last_boundary(jam_file, key, dur, context)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Move estimations to SubSegments folder\n",
      "import glob\n",
      "import os\n",
      "import shutil\n",
      "\n",
      "jam_files = glob.glob(\"/Users/uri/datasets/SubSegments/annotations/*.jams\")\n",
      "for jam_file in jam_files:\n",
      "    orig = \"/Users/uri/datasets/Segments/estimations/\" + os.path.basename(jam_file)[:-5] + \".json\"\n",
      "    dest = \"/Users/uri/datasets/SubSegments/estimations/\" + os.path.basename(orig)\n",
      "    shutil.copy(orig, dest)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Experiment 1: Dur x #bounds x score\n",
      "import sys\n",
      "import jams2\n",
      "import os\n",
      "import glob\n",
      "\n",
      "sys.path.append(\"..\")\n",
      "import eval as EV\n",
      "import msaf_io as MSAF\n",
      "\n",
      "# max_dur: 1728.257007 (SALAMI_878)\n",
      "# max #boundaries: 75 (SALAMI_888)\n",
      "N = 76\n",
      "M = 100\n",
      "# max_M = 1730.\n",
      "max_M = 2000.\n",
      "\n",
      "MAX_N = 138\n",
      "MAX_M = 149\n",
      "\n",
      "algos = [\"olda\", \"siplca\", \"serra\", \"levy\", \"foote\"]\n",
      "trim = False\n",
      "params_dict = {\"olda\" : \"\", \"siplca\" : \"\", \"serra\" : \"mix\",\n",
      "               \"levy\" : \"mfcc\" , \"foote\" : \"mfcc\"}\n",
      "bins = 250\n",
      "\n",
      "in_path = \"/Users/uri/datasets/Segments/\"\n",
      "est_files = glob.glob(os.path.join(in_path, \"estimations\", \"*.json\"))\n",
      "jam_files = glob.glob(os.path.join(in_path, \"annotations\", \"*.jams\"))\n",
      "\n",
      "\n",
      "heat_map = np.zeros((9, N, M))\n",
      "counts = np.zeros((N, M))\n",
      "SD = np.zeros((len(est_files), len(algos), 3)) # Source Data for Eric\n",
      "for i, est_file in enumerate(est_files):\n",
      "    ds_prefix = os.path.basename(est_file).split(\"_\")[0]\n",
      "\n",
      "    # Get corresponding annotation file\n",
      "    jam_file = EV.get_annotation(est_file, jam_files)\n",
      "\n",
      "    # Get number of bounds\n",
      "    try:\n",
      "        ref_inter, ref_labels = jams2.converters.load_jams_range(jam_file,\n",
      "            \"sections\", annotator=0, context=MSAF.prefix_dict[ds_prefix])\n",
      "    except:\n",
      "        print \"No annotation for %s, skipping.\" % jam_file\n",
      "        continue\n",
      "        \n",
      "    n = np.min([len(ref_inter) + 1, MAX_N])\n",
      "    \n",
      "    # Get duration\n",
      "    jam = jams2.load(jam_file)\n",
      "    dur = jam.metadata.duration\n",
      "\n",
      "    # Place duration into correct bin\n",
      "    m = np.min([int(dur / max_M * M), MAX_M])\n",
      "    \n",
      "    # Compute score\n",
      "    score = []\n",
      "    for j, algo_id in enumerate(algos):\n",
      "        params = {\"feature\" : params_dict[algo_id]}\n",
      "        est_inter = MSAF.read_estimations(est_file, algo_id, False, **params)\n",
      "        res = EV.compute_results(ref_inter, est_inter, trim, bins, est_file)\n",
      "        score.append(res)\n",
      "        SD[i, j, :] = np.array([len(ref_inter) + 1, dur, res[2]])\n",
      "    score = np.mean(np.asarray(score), axis=0)\n",
      "    \n",
      "    # Add to heat map\n",
      "    heat_map[:, n, m] += score\n",
      "    counts[n, m] += 1\n",
      "\n",
      "print SD.shape\n",
      "np.save(open(\"exp1.npz\", \"w\"), SD)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_1030.jams\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_1030.jams, skipping.\n",
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_1040.jams"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_1040.jams, skipping.\n",
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_1052.jams"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_1052.jams, skipping.\n",
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_1126.jams"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_1126.jams, skipping.\n",
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_1140.jams"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_1140.jams, skipping.\n",
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_1178.jams"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_1178.jams, skipping.\n",
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_1320.jams"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_1320.jams, skipping.\n",
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_1398.jams"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_1398.jams, skipping.\n",
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_1410.jams"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_1410.jams, skipping.\n",
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_1426.jams"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_1426.jams, skipping.\n",
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_1430.jams\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_1430.jams, skipping.\n",
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_1440.jams"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_1440.jams, skipping.\n",
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_1466.jams"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_1466.jams, skipping.\n",
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_1486.jams"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_1486.jams, skipping.\n",
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_1500.jams"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_1500.jams, skipping.\n",
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_872.jams"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_872.jams, skipping.\n",
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_918.jams"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_918.jams, skipping.\n",
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_966.jams"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_966.jams, skipping.\n",
        "(2174, 5, 3)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 316
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Experiment 1 (cont)\n",
      "\n",
      "metric_dict = {\"F3\" : 2, \"F05\" : 5, \"D\" : 6, \"$\\sigma_{R2E}$\" : 7,\n",
      "               \"$\\sigma_{E2R}$\" : 8}\n",
      "metric = \"$\\sigma_{E2R}$\"\n",
      "\n",
      "# Reduce data\n",
      "mN = 40\n",
      "mM = 40\n",
      "heat_map[np.isnan(heat_map)] = 0\n",
      "heat_map_metric = np.zeros((mN, mM))\n",
      "for i in xrange(heat_map[metric_dict[metric], :, :].shape[0]):\n",
      "    for j in xrange(heat_map[metric_dict[metric], :, :].shape[1]):\n",
      "        heat_map_metric[np.min([i, mN-1]), np.min([j, mM-1])] += heat_map[metric_dict[metric], i, j]\n",
      "idx = np.where(heat_map_metric == 0)\n",
      "heat_map_metric[idx] = np.nan\n",
      "\n",
      "# Plotting\n",
      "figsize = (4, 3)\n",
      "plt.figure(1, figsize=figsize, dpi=120, facecolor='w', edgecolor='k')\n",
      "plt.imshow(heat_map_metric, interpolation=\"nearest\", aspect=\"auto\", cmap=\"hot\")\n",
      "plt.xlabel(\"Duration (seconds)\")\n",
      "plt.ylabel(\"Number of boundaries\")\n",
      "plt.title(\"#boundaries vs duration using %s\" % metric)\n",
      "plt.gcf().subplots_adjust(bottom=0.18)\n",
      "plt.xticks(np.arange(0, mM, 10))\n",
      "plt.gca().set_xticklabels(np.arange(0, mM+1, 10, dtype=int) / float(M) * max_M)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[[ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  ..., \n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
        "\n",
        " [[ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  ..., \n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
        "\n",
        " [[ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  ..., \n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
        "\n",
        " ..., \n",
        " [[ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  ..., \n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
        "\n",
        " [[ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  ..., \n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
        "\n",
        " [[ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  ..., \n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]]]\n"
       ]
      }
     ],
     "prompt_number": 170
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Experiment 2a: Obtain evaluation for all the different subjects\n",
      "import sys\n",
      "sys.path.append(\"..\")\n",
      "import eval as EV\n",
      "\n",
      "reload(EV)\n",
      "\n",
      "def print_results(results, std=False):\n",
      "    \"\"\"Print all the results.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    results: np.array(9)\n",
      "        Results in the following format:\n",
      "            0   :   Precision 3 seconds\n",
      "            1   :   Recall 3 seconds\n",
      "            2   :   F-measure 3 seconds\n",
      "            3   :   Precision 0.5 seconds\n",
      "            4   :   Recall 0.5 seconds\n",
      "            5   :   F-measure 0.5 seconds\n",
      "            6   :   Information Gain\n",
      "            7   :   Median Deviation from Annotated to Estimated boundary\n",
      "            8   :   Median Deviation from Estimated to Annotated boundary\n",
      "    \"\"\"\n",
      "    results = np.asarray(results)\n",
      "    res = results.mean(axis=0)\n",
      "    print \"F3: %.2f, P3: %.2f, R3: %.2f, F05: %.2f, P05: %.2f, \" \\\n",
      "                 \"R05: %.2f, D: %.4f, Ann2EstDev: %.2f, Est2AnnDev: %.2f\" % \\\n",
      "                 (100 * res[2], 100 * res[0], 100 * res[1], 100 * res[5],\n",
      "                  100 * res[3], 100 * res[4], res[6], res[7], res[8])\n",
      "\n",
      "\n",
      "in_path = \"/Users/uri/datasets/SubSegments/\"\n",
      "algos = [\"olda\", \"siplca\", \"serra\", \"levy\", \"foote\"]\n",
      "trim = False\n",
      "params_dict = {\"olda\" : \"\", \"siplca\" : \"\", \"serra\" : \"mix\",\n",
      "               \"levy\" : \"mfcc\" , \"foote\" : \"mfcc\"}\n",
      "annotators = [\"GT\", \"Colin\", \"Eleni\", \"Evan\", \"John\", \"Shuli\"]\n",
      "\n",
      "SD = np.zeros((50, 6, 5, 5)) # Tracks, annotators, Num algos, num metrics\n",
      "F3_F05 = np.zeros((50, 6, 5, 2)) # Tracks, GT+annotators, algorithms, F3 + F05\n",
      "for i, annotator in enumerate(annotators):\n",
      "    res = []\n",
      "    std = []\n",
      "    res_tot = []\n",
      "    for j, algo_id in enumerate(algos):\n",
      "        params = {\"feature\" : params_dict[algo_id]}\n",
      "        results = EV.process(in_path, algo_id, trim=trim, annotator=annotator, \n",
      "                             **params)\n",
      "        SD[:, i, j, :] = results[:, [2,5, 6, 7, 8]]\n",
      "        res_tot.append(results[:, [2,5]])\n",
      "        res.append(np.mean(results, axis=0))\n",
      "        std.append(np.std(results, axis=0))\n",
      "        F3_F05[:, i, j, :] = results[:, [2, 5]]\n",
      "    res_tot = np.asarray(res_tot)\n",
      "    print annotator\n",
      "    print_results(res)\n",
      "    print \"STD\", annotator\n",
      "    print_results(std)\n",
      "\n",
      "F3_F05 = np.asarray(F3_F05)\n",
      "print F3_F05.shape\n",
      "\n",
      "# Save Source Data for Eric-ah\n",
      "print SD.shape\n",
      "np.save(open(\"exp2a.npz\", \"w\"), SD)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "GT\n",
        "F3: 34.66, P3: 37.47, R3: 40.11, F05: 23.17, P05: 24.97, R05: 26.89, D: 0.5242, Ann2EstDev: 6.49, Est2AnnDev: 9.83\n",
        "STD GT\n",
        "F3: 15.74, P3: 21.69, R3: 16.42, F05: 12.02, P05: 15.30, R05: 13.35, D: 0.0786, Ann2EstDev: 7.34, Est2AnnDev: 9.14\n",
        "Colin"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/uri/NYU/Dissertation/mir_eval/beat.py:486: UserWarning: bins parameter is even, so there will not be a bin centered at zero.\n",
        "  warnings.warn(\"bins parameter is even, so there will not be a bin centered at zero.\")\n",
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/estimations/SALAMI_78.json\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/estimations/SALAMI_78.json\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/estimations/SALAMI_78.json\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/estimations/SALAMI_78.json\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/estimations/SALAMI_78.json\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "F3: 40.29, P3: 41.27, R3: 48.27, F05: 26.49, P05: 27.29, R05: 31.45, D: 0.5271, Ann2EstDev: 6.24, Est2AnnDev: 8.37\n",
        "STD Colin\n",
        "F3: 15.06, P3: 19.19, R3: 17.95, F05: 12.41, P05: 14.74, R05: 14.62, D: 0.0795, Ann2EstDev: 6.76, Est2AnnDev: 8.83\n",
        "Eleni"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "F3: 38.47, P3: 41.98, R3: 42.53, F05: 24.62, P05: 27.18, R05: 26.79, D: 0.5134, Ann2EstDev: 6.04, Est2AnnDev: 7.87\n",
        "STD Eleni\n",
        "F3: 15.10, P3: 20.79, R3: 15.93, F05: 11.70, P05: 15.57, R05: 11.91, D: 0.1024, Ann2EstDev: 6.74, Est2AnnDev: 7.87\n",
        "Evan"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "F3: 38.64, P3: 39.20, R3: 45.54, F05: 25.03, P05: 25.90, R05: 29.05, D: 0.5343, Ann2EstDev: 5.31, Est2AnnDev: 9.08\n",
        "STD Evan\n",
        "F3: 15.20, P3: 18.29, R3: 16.90, F05: 11.38, P05: 13.81, R05: 13.26, D: 0.0690, Ann2EstDev: 5.65, Est2AnnDev: 8.17\n",
        "John"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/estimations/Cerulean_Boston_Symphony_Orchestra_&_Charles_Munch-Sympho.json\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/estimations/Cerulean_Boston_Symphony_Orchestra_&_Charles_Munch-Sympho.json\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/estimations/Cerulean_Boston_Symphony_Orchestra_&_Charles_Munch-Sympho.json\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/estimations/Cerulean_Boston_Symphony_Orchestra_&_Charles_Munch-Sympho.json\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/estimations/Cerulean_Boston_Symphony_Orchestra_&_Charles_Munch-Sympho.json\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "F3: 39.02, P3: 36.70, R3: 49.89, F05: 26.59, P05: 25.35, R05: 33.26, D: 0.5446, Ann2EstDev: 6.54, Est2AnnDev: 8.54\n",
        "STD John\n",
        "F3: 15.45, P3: 17.57, R3: 17.85, F05: 12.56, P05: 13.75, R05: 14.60, D: 0.0737, Ann2EstDev: 6.88, Est2AnnDev: 6.04\n",
        "Shuli"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "F3: 38.39, P3: 36.87, R3: 47.99, F05: 25.94, P05: 25.25, R05: 31.65, D: 0.5320, Ann2EstDev: 5.04, Est2AnnDev: 9.86\n",
        "STD Shuli\n",
        "F3: 15.25, P3: 18.05, R3: 16.79, F05: 11.84, P05: 13.55, R05: 12.64, D: 0.1046, Ann2EstDev: 5.32, Est2AnnDev: 11.24\n",
        "(50, 6, 5, 2)\n",
        "(50, 6, 5, 5)\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Experiment 2b: Compare annotations\n",
      "import glob\n",
      "import sys\n",
      "sys.path.append(\"..\")\n",
      "sys.path.append(\"../experiment/\")\n",
      "import eval as EV\n",
      "import analyze_results as AR\n",
      "import json\n",
      "from collections import OrderedDict\n",
      "\n",
      "reload(EV)\n",
      "reload(AR)\n",
      "\n",
      "metric_dict = OrderedDict()\n",
      "metric_dict[\"F3\"] = 2 \n",
      "metric_dict[\"F05\"] = 5 \n",
      "metric_dict[\"D\"] = 6\n",
      "metric_dict[\"$\\sigma_{R2E}$\"] = 7\n",
      "metric_dict[\"$\\sigma_{E2R}$\"] = 8\n",
      "\n",
      "metric = \"F3\"\n",
      "trim = True\n",
      "N = 6 # N annotators\n",
      "X = np.empty((0, N, N))\n",
      "SD = np.zeros([50, 6, 6, 5]) # (num_tracks, num_annotators, num_annotators, metric)\n",
      "jam_files = glob.glob(\"/Users/uri/datasets/SubSegments/annotations/*.jams\")\n",
      "for i, jam_file in enumerate(jam_files):\n",
      "    mma_res = AR.compute_mma_results(jam_file, AR.annotators, trim, gt=True)\n",
      "    x = np.zeros((N, N))\n",
      "    idx = np.triu_indices(N, k=1)\n",
      "    for j, met in enumerate(metric_dict):\n",
      "        x[idx] = mma_res[:, metric_dict[met]]\n",
      "        SD[i, :, :, j] = x\n",
      "    x[idx] = mma_res[:, metric_dict[metric]]\n",
      "    X = np.append(X, [x], axis=0)\n",
      "    \n",
      "# Save data for Ah-Rica\n",
      "print SD.shape\n",
      "np.save(open(\"exp2b.npz\", \"w\"), SD)\n",
      "    \n",
      "X_mean = np.mean(X, axis=0)\n",
      "# X_mean = X_mean + X_mean.T\n",
      "# np.fill_diagonal(X_mean, 1)\n",
      "idx_where = np.where(X_mean == 0)\n",
      "X_mean[idx_where] = np.mean(X_mean[idx])\n",
      "print X_mean\n",
      "vmax = np.sort(np.unique(X_mean))[-2]\n",
      "\n",
      "figsize = (4, 3)\n",
      "annots = [\"GT\", \"Ann1\", \"Ann2\", \"Ann3\", \"Ann4\", \"Ann5\"]\n",
      "plt.figure(1, figsize=figsize, dpi=120, facecolor='w', edgecolor='k')\n",
      "# plt.imshow(X_mean, interpolation=\"nearest\", aspect=\"auto\", cmap=\"hot\", vmin=X_mean.min(), vmax=vmax)\n",
      "plt.imshow(X_mean, interpolation=\"nearest\", aspect=\"auto\", cmap=\"hot\")\n",
      "print json.dumps(X_mean.tolist())\n",
      "plt.gca().set_xticks(np.arange(0,6))\n",
      "plt.gca().set_yticks(np.arange(0,6))\n",
      "plt.gca().set_xticklabels(annots)\n",
      "plt.gca().set_yticklabels(annots)\n",
      "# plt.title(\"Annotators Agreement for %s\" % metric)\n",
      "plt.title(\"Annotators Mutual Agreement\")\n",
      "plt.colorbar()\n",
      "plt.show()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/uri/NYU/Dissertation/mir_eval/beat.py:486: UserWarning: bins parameter is even, so there will not be a bin centered at zero.\n",
        "  warnings.warn(\"bins parameter is even, so there will not be a bin centered at zero.\")\n",
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/annotations/Cerulean_Boston_Symphony_Orchestra_&_Charles_Munch-Sympho.jams\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/annotations/Cerulean_Boston_Symphony_Orchestra_&_Charles_Munch-Sympho.jams\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/annotations/Cerulean_Boston_Symphony_Orchestra_&_Charles_Munch-Sympho.jams\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/annotations/Cerulean_Boston_Symphony_Orchestra_&_Charles_Munch-Sympho.jams\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/annotations/Cerulean_Boston_Symphony_Orchestra_&_Charles_Munch-Sympho.jams\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/annotations/SALAMI_78.jams\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/annotations/SALAMI_78.jams\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/annotations/SALAMI_78.jams\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/annotations/SALAMI_78.jams\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/annotations/SALAMI_78.jams\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(50, 6, 6, 5)\n",
        "[[ 0.58803096  0.58580553  0.60274822  0.62862279  0.54045208  0.5752257 ]\n",
        " [ 0.58803096  0.58803096  0.61170652  0.59340759  0.56479887  0.64702824]\n",
        " [ 0.58803096  0.58803096  0.58803096  0.60483826  0.52665242  0.59557561]\n",
        " [ 0.58803096  0.58803096  0.58803096  0.58803096  0.54361601  0.62365065]\n",
        " [ 0.58803096  0.58803096  0.58803096  0.58803096  0.58803096  0.57633595]\n",
        " [ 0.58803096  0.58803096  0.58803096  0.58803096  0.58803096  0.58803096]]\n",
        "[[0.5880309622668793, 0.5858055320534442, 0.6027482197457384, 0.6286227877765598, 0.5404520816680207, 0.5752256991571328], [0.5880309622668793, 0.5880309622668793, 0.6117065162573256, 0.5934075911739257, 0.564798874629181, 0.6470282354474653], [0.5880309622668793, 0.5880309622668793, 0.5880309622668793, 0.6048382612398049, 0.5266524244063607, 0.5955756102325015], [0.5880309622668793, 0.5880309622668793, 0.5880309622668793, 0.5880309622668793, 0.5436160053470147, 0.623650649252582], [0.5880309622668793, 0.5880309622668793, 0.5880309622668793, 0.5880309622668793, 0.5880309622668793, 0.5763359456161314], [0.5880309622668793, 0.5880309622668793, 0.5880309622668793, 0.5880309622668793, 0.5880309622668793, 0.5880309622668793]]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAREAAADQCAYAAAAgV1UrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCCFqoSBCCFHL/wF8AfBIKnCfxQAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x105237910>"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Experiment 3: compare all the annotated boundaries to come up with a more robust reference\n",
      "import glob\n",
      "import sys\n",
      "import os\n",
      "sys.path.append(\"..\")\n",
      "sys.path.append(\"../experiment/\")\n",
      "\n",
      "import eval as EV\n",
      "import analyze_results as AR\n",
      "import msaf_io as MSAF\n",
      "import json\n",
      "\n",
      "reload(EV)\n",
      "reload(AR)\n",
      "\n",
      "def create_weighted_annot(jam, weights, histo_times, jam_file):\n",
      "    # Annot\n",
      "    annot_name = \"Weighted\"\n",
      "    annot = None\n",
      "    for annotation in jam.sections:\n",
      "        if annotation.annotation_metadata.annotator.name == annot_name:\n",
      "            annotation.data = []\n",
      "            annot = annotation\n",
      "            break\n",
      "    if annot is None:\n",
      "        annot = jam.sections.create_annotation()\n",
      "        annot.annotation_metadata.origin = \"Synth\"\n",
      "        annot.annotation_metadata.annotator.name = annot_name\n",
      "        annot.annotation_metadata.annotator.email = \"\"\n",
      "    \n",
      "    bound_times = []\n",
      "    bound_weights = []\n",
      "    for i, weight in enumerate(weights):\n",
      "        if weight != 0:\n",
      "            bound_times.append(histo_times[i])\n",
      "            bound_weights.append(weights[i])\n",
      "    for i, bound in enumerate(bound_times):\n",
      "        if i == 0:\n",
      "            continue\n",
      "        section = annot.create_datapoint()\n",
      "        section.start.value = bound_times[i-1]\n",
      "        section.start.confidence = bound_weights[i-1]\n",
      "        section.end.value = bound_times[i]\n",
      "        section.end.confidence = bound_weights[i]\n",
      "        section.label.context = \"synth\"\n",
      "    \n",
      "    json.dump(jam, open(jam_file, \"w\"), indent=2)\n",
      "    \n",
      "\n",
      "def create_thres_annot(jam, thresh_bounds, thresh_conf, jam_file):\n",
      "    # Annot\n",
      "    annot_name = \"Threshold\"\n",
      "    annot = None\n",
      "    for annotation in jam.sections:\n",
      "        if annotation.annotation_metadata.annotator.name == annot_name:\n",
      "            annotation.data = []\n",
      "            annot = annotation\n",
      "            break\n",
      "    if annot is None:\n",
      "        annot = jam.sections.create_annotation()\n",
      "        annot.annotation_metadata.origin = \"Synth\"\n",
      "        annot.annotation_metadata.annotator.name = annot_name\n",
      "        annot.annotation_metadata.annotator.email = \"\"\n",
      "    \n",
      "    for i, bound in enumerate(thresh_bounds):\n",
      "        if i == 0:\n",
      "            continue\n",
      "        section = annot.create_datapoint()\n",
      "        section.start.value = thresh_bounds[i-1]\n",
      "        section.start.confidence = thresh_conf[i-1]\n",
      "        section.end.value = thresh_bounds[i]\n",
      "        section.end.confidence = thresh_conf[i]\n",
      "        section.label.context = \"synth\"\n",
      "    \n",
      "    json.dump(jam, open(jam_file, \"w\"), indent=2)\n",
      "\n",
      "    \n",
      "annotators = [\"GT\", \"Colin\", \"Eleni\", \"Evan\", \"John\", \"Shuli\"]\n",
      "histo_bins = 300\n",
      "N = len(annotators)\n",
      "X = np.empty((0, N, N))\n",
      "jam_files = glob.glob(\"/Users/uri/datasets/SubSegments/annotations/*.jams\")\n",
      "for jam_file in jam_files[15:]:\n",
      "    # Get Duration\n",
      "    jam = jams2.load(jam_file)\n",
      "    dur = jam.metadata.duration\n",
      "    \n",
      "    # Compute the weighted boundaries\n",
      "    boundaries = []\n",
      "    ds_prefix = os.path.basename(jam_file).split(\"_\")[0]\n",
      "    for annot in annotators:\n",
      "        if annot == \"GT\":\n",
      "            ann_inter, ann_labels = jams2.converters.load_jams_range(jam_file,\n",
      "                \"sections\", annotator=0, context=MSAF.prefix_dict[ds_prefix])\n",
      "        else:\n",
      "            ann_inter, ann_labels = jams2.converters.load_jams_range(jam_file,\n",
      "                \"sections\", annotator_name=annot, context=\"large_scale\")\n",
      "        ann_times = EV.intervals_to_times(ann_inter)\n",
      "        histo_bounds, histo_times = np.histogram(ann_times, bins=histo_bins, range=(0, dur))\n",
      "        boundaries.append(histo_bounds)\n",
      "    \n",
      "    weights = np.mean(boundaries, axis=0)\n",
      "    weights /= weights.max()\n",
      "    \n",
      "    print jam_file\n",
      "    f, axarr = plt.subplots(2, sharex=False, figsize=(7,3.5))\n",
      "#     axarr[0].plot(x, y)\n",
      "#     axarr[0].set_title('Sharing X axis')\n",
      "#     axarr[1].scatter(x, y)\n",
      "    AR.plot_ann_boundaries(jam_file, AR.annotators, ax=axarr[0])\n",
      "    plt.suptitle(\"Merging Human Boundaries\")\n",
      "    axarr[1].plot(weights)\n",
      "    plt.xlim(0, 308)\n",
      "    axarr[1].set_xticks(np.arange(0, 308, 308/5))\n",
      "    axarr[1].set_xticklabels(np.arange(0, 251, 50))\n",
      "    axarr[1].set_ylabel(\"Weight\")\n",
      "    axarr[1].set_xlabel(\"Duration (seconds)\")\n",
      "    plt.subplots_adjust(bottom=0.2)\n",
      "    sys.exit()\n",
      "    \n",
      "    # Save weighted boundaries\n",
      "    create_weighted_annot(jam, weights, histo_times, jam_file)\n",
      "    \n",
      "    # Compoute the thresholded boundaries\n",
      "    L = 15\n",
      "    hann = np.hanning(L)\n",
      "    weighted_filt = np.convolve(weights, hann, mode=\"same\")\n",
      "    weighted_filt /= weighted_filt.max()\n",
      "    thresh_bounds = []\n",
      "    thresh_conf = []\n",
      "    for i, weight in enumerate(weighted_filt):\n",
      "        peak_found = False\n",
      "        if i == 0 and weighted_filt[i+1] < weight:\n",
      "            peak_found = True\n",
      "        elif i == len(weighted_filt) - 1 and weighted_filt[i-1] < weight:\n",
      "            peak_found = True\n",
      "        elif i != 0 and i != len(weighted_filt) - 1 and \\\n",
      "                weighted_filt[i-1] < weight and weight > weighted_filt[i+1]:\n",
      "            peak_found = True\n",
      "        if peak_found:\n",
      "            thresh_bounds.append(histo_times[i])\n",
      "            thresh_conf.append(weight)\n",
      "\n",
      "    # Save \n",
      "    create_thres_annot(jam, thresh_bounds, thresh_conf, jam_file)\n",
      "    \n",
      "#     plt.plot(weighted_filt)\n",
      "#     plt.show()\n",
      "#     sys.exit()\n",
      "\n",
      "                \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/Users/uri/datasets/SubSegments/annotations/Isophonics_09_-_You_Never_Give_Me_Your_Money.jams\n",
        "[  0.00000000e+00   1.80000000e-01   6.98480000e+01   1.48093000e+02\n",
        "   2.42400000e+02]"
       ]
      },
      {
       "ename": "SystemExit",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "An exception has occurred, use %tb to see the full traceback.\n",
        "\u001b[0;31mSystemExit\u001b[0m\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[  1.85759637e-01   2.34985941e+01   6.99385034e+01   9.26011791e+01\n",
        "   1.08019229e+02   1.48236190e+02   1.83344762e+02   2.42415600e+02]\n",
        "[   0.           23.31283447   70.33324263  100.12444444  130.93732426\n",
        "  148.46839002  230.01687075  242.4156    ]\n",
        "[   0.            0.38603175   70.26938776   92.32253968  130.35102041\n",
        "  148.79346939  189.19619048  242.4156    ]\n",
        "[   0.           68.26666667   91.95102041  132.63238095  159.10312925\n",
        "  183.62340136  230.15619048  242.4156    ]\n",
        "[  1.30612245e-01   6.99428571e+01   9.18204082e+01   1.08212245e+02\n",
        "   1.48114286e+02   2.42351020e+02]\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "To exit: use 'exit', 'quit', or Ctrl-D.\n"
       ]
      }
     ],
     "prompt_number": 616
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Experiment 3a (cont)\n",
      "# Now we already have a better reference, let's compute the numbers on the Weighted result\n",
      "import jams2\n",
      "import os\n",
      "import sys\n",
      "import mir_eval\n",
      "\n",
      "sys.path.append(\"..\")\n",
      "import eval as EV\n",
      "\n",
      "reload(jams2)\n",
      "reload(jams2.converters)\n",
      "\n",
      "\n",
      "def weighted_hit_rate(ref_inters, est_inters, weights_inters, window=3):\n",
      "    ref = EV.intervals_to_times(ref_inters)\n",
      "    est = EV.intervals_to_times(est_inters)\n",
      "    weights = EV.intervals_to_times(np.asarray(weights_inters))\n",
      "    \n",
      "    # Find matches\n",
      "    matching    = mir_eval.util.match_events(ref,est,window)\n",
      "    \n",
      "    # Apply weights\n",
      "    hits = np.zeros((len(matching)))\n",
      "    for i in xrange(len(matching)):\n",
      "        hits[i] = weights[matching[i][1]]\n",
      "    \n",
      "    # Compute the precision denominator (if hit not found, take mean of weights)\n",
      "    denom_prec = np.ones(len(est)) * np.mean(weights)\n",
      "    for i in xrange(len(matching)):\n",
      "        denom_prec[matching[i][0]] = weights[matching[i][1]]\n",
      "    \n",
      "    # Compute scores\n",
      "    precision   = np.sum(hits) / np.sum(denom_prec)\n",
      "    recall      = np.sum(hits) / np.sum(weights)\n",
      "    f           = mir_eval.util.f_measure(precision, recall)\n",
      "    \n",
      "    return precision, recall, f\n",
      "    \n",
      "\n",
      "jam_files = glob.glob(\"/Users/uri/datasets/SubSegments/annotations/*.jams\")\n",
      "est_files = glob.glob(\"/Users/uri/datasets/SubSegments/estimations/*.json\")\n",
      "algos = [\"olda\", \"siplca\", \"serra\", \"levy\", \"foote\"]\n",
      "trim = False\n",
      "params_dict = {\"olda\" : \"\", \"siplca\" : \"\", \"serra\" : \"mix\",\n",
      "               \"levy\" : \"mfcc\" , \"foote\" : \"mfcc\"}\n",
      "\n",
      "F3_tot = []\n",
      "F05_tot = []\n",
      "SD = np.zeros((50, 7, 5, 2)) # tracks, annotators + merged, algorithms, F3 + F05\n",
      "for i, jam_file in enumerate(jam_files):\n",
      "    est_file = est_files[i]\n",
      "    assert os.path.basename(jam_file)[:-5] == os.path.basename(est_file[:-5])\n",
      "    ref_inters, ref_labels, ref_conf = jams2.converters.load_jams_range(jam_file,\n",
      "            \"sections\", annotator_name=\"Weighted\", context=\"synth\", confidence=True)\n",
      "    F3 = []\n",
      "    F05 = []\n",
      "    for j, algo_id in enumerate(algos):\n",
      "        params = {\"feature\" : params_dict[algo_id]}\n",
      "        est_inters = MSAF.read_estimations(est_file, algo_id, False, **params)\n",
      "        p3, r3, f3 = weighted_hit_rate(ref_inters, est_inters, ref_conf, window=3)\n",
      "        F3.append(f3)\n",
      "        p05, r05, f05 = weighted_hit_rate(ref_inters, est_inters, ref_conf, window=0.5)\n",
      "        F05.append(f05)\n",
      "        SD[i, 6, j, :] = np.asarray([f3, f05])\n",
      "        \n",
      "    F3_tot.append(np.mean(F3))\n",
      "    F05_tot.append(np.mean(F05))\n",
      "    \n",
      "# Assign teh values for the other annotators\n",
      "SD[:, :-1, :, :] = F3_F05[:, :, :, :]\n",
      "\n",
      "# Save data for Eric guapo\n",
      "print SD.shape\n",
      "np.save(open(\"exp3a.npz\", \"w\"), SD)\n",
      "\n",
      "print \"F3 (weighted):\", np.mean(F3_tot), np.std(F3_tot)\n",
      "print \"F05 (weighted):\", np.mean(F05_tot), np.std(F05_tot)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(50, 7, 5, 2)\n",
        "F3 (weighted): 0.438662660079 0.0687507479344\n",
        "F05 (weighted): 0.284668254011 0.0983049649893\n"
       ]
      }
     ],
     "prompt_number": 363
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Experiment 3b\n",
      "# Thresholded version of merged boundaries\n",
      "import jams2\n",
      "import os\n",
      "import sys\n",
      "import mir_eval\n",
      "\n",
      "sys.path.append(\"..\")\n",
      "import eval as EV\n",
      "\n",
      "reload(jams2)\n",
      "reload(jams2.converters)\n",
      "\n",
      "jam_files = glob.glob(\"/Users/uri/datasets/SubSegments/annotations/*.jams\")\n",
      "est_files = glob.glob(\"/Users/uri/datasets/SubSegments/estimations/*.json\")\n",
      "algos = [\"olda\", \"siplca\", \"serra\", \"levy\", \"foote\"]\n",
      "trim = False\n",
      "window = 3\n",
      "params_dict = {\"olda\" : \"\", \"siplca\" : \"\", \"serra\" : \"mix\",\n",
      "               \"levy\" : \"mfcc\" , \"foote\" : \"mfcc\"}\n",
      "\n",
      "def filter_ref(ref_inters, ref_conf, th):\n",
      "    assert len(ref_inters) == len(ref_conf)\n",
      "    ref_times = EV.intervals_to_times(ref_inters)\n",
      "    conf_times = EV.intervals_to_times(ref_conf)\n",
      "    idx = np.argwhere(conf_times >= th)\n",
      "    ref_times = ref_times[idx].flatten()\n",
      "    return EV.times_to_intervals(ref_times)\n",
      "\n",
      "F3_th = []\n",
      "F05_th = []\n",
      "SD = np.zeros([50, 5, 20, 2]) # num_tracks, num_algorithms, num_thresh, F-measures\n",
      "for i, th in enumerate(np.arange(0, 1, .05)):\n",
      "    F3_tot = []\n",
      "    F05_tot = []\n",
      "    print \"computing \", th\n",
      "    j = 0\n",
      "    for jam_file, est_file in zip(jam_files, est_files):\n",
      "        assert os.path.basename(jam_file)[:-5] == os.path.basename(est_file[:-5])\n",
      "        ref_inters, ref_labels, ref_conf = jams2.converters.load_jams_range(jam_file,\n",
      "                \"sections\", annotator_name=\"Weighted\", context=\"synth\", confidence=True)\n",
      "        ref_inters = filter_ref(ref_inters, np.asarray(ref_conf), th)\n",
      "        \n",
      "        F3 = []\n",
      "        F05 = []\n",
      "        if len(ref_inters) == 0:\n",
      "            F3 = [0]\n",
      "            F05 = [0]\n",
      "            SD[j, :, i, :] = 0\n",
      "        else:\n",
      "            for k, algo_id in enumerate(algos):\n",
      "                params = {\"feature\" : params_dict[algo_id]}\n",
      "                est_inters = MSAF.read_estimations(est_file, algo_id, False, **params)\n",
      "                p, r, f = mir_eval.boundary.detection(ref_inters, est_inters, window=3)\n",
      "                F3.append(f)\n",
      "                p, r, f = mir_eval.boundary.detection(ref_inters, est_inters, window=0.5)\n",
      "                F05.append(f)\n",
      "                SD[j, k, i, :] = np.asarray([F3[-1], F05[-1]])\n",
      "        F3_tot.append(np.mean(F3))\n",
      "        F05_tot.append(np.mean(F05))\n",
      "        j += 1\n",
      "    F3_th.append(np.mean(F3_tot))\n",
      "    F05_th.append(np.mean(F05_tot))\n",
      "    \n",
      "print SD.shape\n",
      "np.save(open(\"exp3b.npz\", \"w\"), SD)\n",
      "\n",
      "# Plot\n",
      "figsize = (4, 3)\n",
      "plt.figure(1, figsize=figsize, dpi=120, facecolor='w', edgecolor='k')\n",
      "plt.plot(np.arange(0, 1, .05), F3_th, \"b--\", label='$F_3$')\n",
      "plt.plot(np.arange(0, 1, .05), F05_th, \"r-\", label='$F_{0.5}$')\n",
      "plt.title(\"Thresholded aggregation\")\n",
      "plt.xlabel(\"Threshold\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.gca().legend(loc='upper right', shadow=True)\n",
      "plt.gcf().subplots_adjust(bottom=0.16, left=.15)\n",
      "plt.show()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "computing  0.0\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.05\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.1\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.15\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.2\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.25\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.3\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.35\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.4\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.45\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.5\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.55\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.6\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.65\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.7\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.75\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.8\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.85\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.9\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.95\n",
        "(50, 5, 20, 2)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 360
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Experiment 4a\n",
      "import json\n",
      "from collections import OrderedDict\n",
      "\n",
      "tags = json.load(open(\"../experiment/results/merged_tags_ejh_resolved.json\", \"r\"))\n",
      "tags = OrderedDict(sorted(tags.items(), key=lambda t: t[0]))\n",
      "num_tags = []\n",
      "for i, t in enumerate(tags):\n",
      "    num_tags.append(len(np.where(np.asarray(tags[t]) != \"\")[0]))\n",
      "    print i, t\n",
      "# Plot number of annotators who reported at least one track\n",
      "figsize = (6, 3)\n",
      "plt.figure(1, figsize=figsize, dpi=120, facecolor='w', edgecolor='k')\n",
      "# plt.axvspan(8, 10.8, color='g', alpha=0.5)\n",
      "# plt.axvspan(14, 14.8, color='g', alpha=0.5)\n",
      "# plt.axvspan(37, 37.8, color='g', alpha=0.5)\n",
      "plt.axvspan(0, 2.8, color='g', alpha=0.5)\n",
      "plt.bar(np.arange(len(num_tags)), np.sort(num_tags))\n",
      "plt.title(\"Analysis of difficulty from Human POV\")\n",
      "plt.axvspan(11, 12.8, color='g', alpha=0.5, label=\"Easy Tracks\\nfrom Machine POV\")\n",
      "plt.xlabel(\"Tracks sorted by number of annotators who reported at least one tag\")\n",
      "plt.ylabel(\"Number of annotators\\nwho reported at least one tag\", multialignment='center')\n",
      "plt.gca().legend(loc='lower right', shadow=True, prop={'size':11})\n",
      "plt.gcf().subplots_adjust(bottom=0.18)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 Cerulean_Bob_Dylan-Hurricane.mp3\n",
        "1 Cerulean_Bob_Dylan-Like_a_Rolling_Stone_(Live).mp3\n",
        "2 Cerulean_Boston_Symphony_Orchestra_&_Charles_Munch-Sympho.mp3\n",
        "3 Cerulean_Eddie_Palmieri-Adoracion.mp3\n",
        "4 Cerulean_Leonard_Bernstein,_New_York_Philharmonic_&_Rudol.mp3\n",
        "5 Cerulean_Miles_Davis_Quintet-Footprints.mp3\n",
        "6 Cerulean_Prince_&_The_Revolution-Purple_Rain.mp3\n",
        "7 Cerulean_Yes-Starship_Trooper/_A._Life_Seeker,_B._Disillu.mp3\n",
        "8 Epiphyte_0220_promiscuous.mp3\n",
        "9 Epiphyte_0723_hotelroomservice.mp3\n",
        "10 Epiphyte_0780_letmebereal.mp3\n",
        "11 Isophonics_01 The Show Must Go On.mp3\n",
        "12 Isophonics_02 Under Pressure.mp3\n",
        "13 Isophonics_05 Somebody To Love.mp3\n",
        "14 Isophonics_07_-_Maggie_Mae.mp3\n",
        "15 Isophonics_09_-_You_Never_Give_Me_Your_Money.mp3\n",
        "16 Isophonics_CD1_-_14_-_Don't_Pass_Me_By.mp3\n",
        "17 Isophonics_CD2_-_06_-_Helter_Skelter.mp3\n",
        "18 SALAMI_1072.mp3\n",
        "19 SALAMI_108.mp3\n",
        "20 SALAMI_114.mp3\n",
        "21 SALAMI_1170.mp3\n",
        "22 SALAMI_1198.mp3\n",
        "23 SALAMI_12.mp3\n",
        "24 SALAMI_1240.mp3\n",
        "25 SALAMI_1324.mp3\n",
        "26 SALAMI_1482.mp3\n",
        "27 SALAMI_1562.mp3\n",
        "28 SALAMI_164.mp3\n",
        "29 SALAMI_20.mp3\n",
        "30 SALAMI_278.mp3\n",
        "31 SALAMI_286.mp3\n",
        "32 SALAMI_302.mp3\n",
        "33 SALAMI_308.mp3\n",
        "34 SALAMI_444.mp3\n",
        "35 SALAMI_458.mp3\n",
        "36 SALAMI_478.mp3\n",
        "37 SALAMI_538.mp3\n",
        "38 SALAMI_542.mp3\n",
        "39 SALAMI_546.mp3\n",
        "40 SALAMI_584.mp3\n",
        "41 SALAMI_68.mp3\n",
        "42 SALAMI_728.mp3\n",
        "43 SALAMI_78.mp3\n",
        "44 SALAMI_812.mp3\n",
        "45 SALAMI_830.mp3\n",
        "46 SALAMI_838.mp3\n",
        "47 SALAMI_876.mp3\n",
        "48 SALAMI_940.mp3\n",
        "49 SALAMI_944.mp3\n"
       ]
      }
     ],
     "prompt_number": 572
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Experiment 4b\n",
      "from collections import Counter\n",
      "\n",
      "tag_groups = [\"annotator\", \"audio_quality\", \"form\", \"instrumentation\", \"style\"]\n",
      "tag_names = np.asarray([\"Annotator\", \"Audio Quality\", \"Form\", \"Instrumentation\", \"Style\"])\n",
      "tag_frequency = np.zeros(len(tag_groups), dtype=int)\n",
      "count_tags = []\n",
      "for key, ann_tags in tags.iteritems():\n",
      "    for tag in ann_tags:\n",
      "        for real_tag in tag.split(\",\"):\n",
      "            if real_tag != \"\":\n",
      "                count_tags.append(real_tag)\n",
      "            for i, tag_group in enumerate(tag_groups):\n",
      "                if real_tag.split(\"-\")[0] in tag_group and real_tag.split(\"-\")[0] != \"\":\n",
      "                    tag_frequency[i] += 1\n",
      "\n",
      "print tag_frequency\n",
      "counter = Counter(count_tags)\n",
      "print counter\n",
      "\n",
      "# Sort tags\n",
      "idx = np.argsort(tag_frequency)\n",
      "tag_frequency = tag_frequency[idx]\n",
      "tag_names = tag_names[idx]\n",
      "\n",
      "# Plot\n",
      "figsize = (6, 3)\n",
      "plt.figure(1, figsize=figsize, dpi=120, facecolor='w', edgecolor='k')\n",
      "plt.barh(np.arange(len(tag_frequency)), tag_frequency, align=\"center\")\n",
      "plt.yticks(np.arange(len(tag_frequency)))\n",
      "plt.gca().set_yticklabels(tag_names)\n",
      "plt.gcf().subplots_adjust(bottom=0.17, left=0.25)\n",
      "plt.xlabel(\"Number of tags\")\n",
      "plt.title(\"Difficult Tags Grouped by Type\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 7  3 41  9 42]\n",
        "Counter({u'form-varied': 14, u'style-complex': 10, u'form-long': 10, u'form-repetitive': 9, u'annotator-clueless': 7, u'style-improvisation': 7, u'style-constant': 6, u'style-slow': 5, u' form-varied': 5, u' annotator-clueless': 5, u'style-embellishments': 5, u' style-improvisation': 5, u'instrumentation-solo': 5, u' form-long': 4, u'form-complex': 4, u' style-fast': 3, u' style-embellishments': 3, u'audio_quality-noisy': 2, u' form-complex': 2, u'style-complex(rhythm)': 2, u' style-complex': 2, u'form-short': 2, u'style-theatrical': 2, u'instrumentation-solo(voice)': 2, u'style-fast': 2, u'style-complex(harmony)': 2, u'form-long(sections)': 2, u' instrumentation-no_voice': 1, u'instrumentation-solo(guitar)': 1, u'instrumentation-no_voice': 1, u'style-jazz': 1, u' audio_quality-muffled': 1, u' form-repetitive': 1, u'audio_quality-muffled': 1, u' style-jazz': 1, u' style-complex(harmony)': 1})\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 470,
       "text": [
        "<matplotlib.text.Text at 0x119b3df10>"
       ]
      }
     ],
     "prompt_number": 470
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "\n",
      "COLORS = np.asarray([\"b\", \"g\", \"r\", \"m\", \"y\", \"c\"])\n",
      "ANNOTS = np.asarray([\"GT\", \"Ann1\", \"Ann2\", \"Ann3\", \"Ann4\", \"Ann5\"])\n",
      "\n",
      "SD = np.load(\"/Users/uri/Dropbox/NYU/Publications/ISMIR2014-NietoHumphreyFarboodBello/exp2b.npz\")\n",
      "num_trakcs = SD.shape[0]  #(num_tracks, num_annotators, num_annotators, metric)\n",
      "num_annots = SD.shape[1]  # or SD.shape[2]\n",
      "SD_F3 = SD[:, :, :, 0]    # Get only the F3\n",
      "print num_metrics\n",
      "\n",
      "# Copy the elements from the upper triangle to the lower\n",
      "for i in range(num_annots):\n",
      "    for j in range(num_annots):\n",
      "        SD[:, j, i] = SD[:, i, j]\n",
      "\n",
      "# Setup the figure\n",
      "figsize = (6, 3)\n",
      "plt.figure(1, figsize=figsize, dpi=120, facecolor='w', edgecolor='k')\n",
      "\n",
      "# Main loop to plot the bars\n",
      "N = 8\n",
      "for i in range(num_annots):\n",
      "    # Get indices to select for the colors and the annotators (except the current one)\n",
      "    all_idxs = np.arange(num_annots)\n",
      "    idxs = np.delete(all_idxs, i)\n",
      "    color = COLORS[idxs]\n",
      "    labels = ANNOTS[idxs]\n",
      "    \n",
      "    # Get all the means for all the annotators (except the current one)\n",
      "    annot = np.asarray([np.mean(x) for x in SD_F3.swapaxes(2,0)[idxs, i, :]])\n",
      "    \n",
      "    # Sort\n",
      "    sort_idxs = np.argsort(annot)\n",
      "#     annot = annot[sort_idxs]\n",
      "#     color = color[sort_idxs]\n",
      "#     labels = labels[sort_idxs]\n",
      "    \n",
      "    # Plot\n",
      "    if i != 0:\n",
      "        last_bar = bars[-1]\n",
      "    bars = plt.bar(np.arange(i*N, i*N+len(annot)), annot, width=1, color=color, align=\"center\")\n",
      "\n",
      "bars += (last_bar,)\n",
      "labels = tuple(labels) + (\"Ann5\",)\n",
      "# plt.legend((bars[0], bars[4], bars[2], bars[3], bars[1], bars[5]), \n",
      "#            (labels[0], labels[4], labels[2], labels[3], labels[1], labels[5]) , loc='center right')\n",
      "plt.legend(bars, labels, loc='center right')\n",
      "plt.xticks(np.arange(num_annots/2-1, N*num_annots, N))\n",
      "plt.gca().set_xticklabels(ANNOTS)\n",
      "plt.gca().set_ylim(0.45, 0.7)\n",
      "plt.gca().set_xlim(-1, N*num_annots + 18)\n",
      "plt.ylabel(\"Hit Rate F-measure\")\n",
      "plt.title(\"Agreement between Human Annotations\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 571,
       "text": [
        "<matplotlib.text.Text at 0x11a877f50>"
       ]
      }
     ],
     "prompt_number": 571
    }
   ],
   "metadata": {}
  }
 ]
}